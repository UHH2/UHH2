# This is a small example how the crab api can easily be used to create something like multi crab.
# It has some additional features like also creating the xml files for you.
# For it to work you need inputDatasets & requestNames apart from the classical part
#
# Make sure to have a unique directory where your joboutput is saved, otherwise the script gets confused and you too!!
#
# Usage ./CrabConfig ConfigFile [options]
#
# Take care here to make the request names *nice*
#
# autocomplete_Datasets(ListOfDatasets) works also for several entries with *


import re
from DasQuery import autocomplete_Datasets


def get_request_name(dataset_name):
    """Generate short string to use for request name from full dataset name

    Note that since this is used later on by e.g. multicrab status check,
    it should be invariant wrt time, commit hash, etc, otherwise it will not
    find the correct dir

    The short string generated by this function should however still contain as much information
    as needed so that people later on know which sample it is in CMS DAS (https://cmsweb.cern.ch/das)
    """

    is_real_data = dataset_name.split('/')[-1] == 'MINIAOD' # would be MINIAODSIM for MC
    # for real data, dataset names are usually not very long; thus, keep the full dataset name (except the obvious "MINIAOD" at the end) to prevent any confusion
    if is_real_data:
        modified_name = '_'.join(dataset_name.split('/')[1:-1])
        return modified_name

    # for the usually very long MC dataset names, do some more name modification ...

    modified_name = dataset_name.split('/')[1]
    modified_name = modified_name.replace('_TuneCUETP8M1_13TeV-madgraphMLM-pythia8', '_P8M1')
    modified_name = modified_name.replace('_TuneCP5_13TeV-madgraphMLM-pythia8', '_CP5')
    modified_name = modified_name.replace('_TuneCUETP8M1_13TeV_pythia8', '_P8M1')

    # request name can only be 100 characters maximum
    # at this point we need to chop it down, to allow for campaign, time, date, ext2, v2
    max_len = 100-34
    if len(modified_name) > max_len:
        modified_name = modified_name[:max_len]

    # Add MC campaign
    if "Summer16" in dataset_name:
        modified_name += "_Summer16"
    elif "Fall17" in dataset_name:
        modified_name += "_Fall17"
    elif "Autumn18" in dataset_name:
        modified_name += "_Autumn18"
    elif "Summer19UL17" in dataset_name:
        modified_name += "_Summer19UL17"
    elif "Summer19UL18" in dataset_name:
        modified_name += "_Summer19UL18"
    elif "Summer20UL16" in dataset_name:
        modified_name += "_Summer20UL16"
        if "APV" in dataset_name:
            modified_name += "APV"
    elif "Summer20UL17" in dataset_name:
        modified_name += "_Summer20UL17"
    elif "Summer20UL18" in dataset_name:
        modified_name += "_Summer20UL18"

    ext = re.search('ext[0-9]+', dataset_name.split('/')[-2])
    if ext:
        modified_name += '_'+ext.group(0)
    elif 'ext' in dataset_name:
        modified_name += '_ext'
    elif 'backup' in dataset_name:
        modified_name += '_backup'

    version_number = dataset_name.split('/')[-2].split('-')[-1]
    if re.match('v[0-9]+', version_number):
        modified_name += '_'+version_number

    return modified_name


inputDatasets = ['/DYJetsToLL_M-50_HT-*to*_TuneCUETP8M1_13TeV-madgraphMLM-pythia8/RunIISummer16MiniAODv2-PUMoriond17_80X_mcRun2_asymptotic_2016_TrancheIV_v6_*/MINIAODSIM']
inputDatasets = autocomplete_Datasets(inputDatasets)
requestNames = [get_request_name(x) for x in inputDatasets]

# ===============================================================================
# Classical part of crab, after resolving the * it uses in the example below just the first entry
#

from CRABClient.UserUtilities import config
from CRABClient.ClientExceptions import ProxyException
import os
import re


config = config()
config.General.workArea = 'crab_Test' # Don't forget to adjust this
config.General.transferOutputs = True
config.General.transferLogs = True

config.JobType.pluginName = 'Analysis'
config.JobType.psetName = os.path.join(os.environ['CMSSW_BASE'], 'src/UHH2/core/python/ntuplewriter_mc_2016.py') # Don't forget to adjust this
config.JobType.outputFiles = ["Ntuple.root"]
config.JobType.maxMemoryMB = 2500

config.Data.inputDBS = 'global'
# Recommended for real data (don't use the 'Automatic' splitting as recommended in the crab TWiki; it sometimes bugs out and leads to duplicate data -- an absolute No-Go!):
#config.Data.splitting = 'EventAwareLumiBased'
#config.Data.unitsPerJob = 24000 # targeted approximate number of events per job
# Recommended for MC:
config.Data.splitting = 'FileBased'
config.Data.unitsPerJob = 1 # number of files processed per job

# Add subdirectory using year from config filename
pset = os.path.basename(config.JobType.psetName)
result = re.search(r'(20|UL)1[\d](v\d)?', pset)
if not result:
    raise RuntimeError("Cannot extract year from psetName! Does your psetName have 201* in it (or UL1*)?")
year = result.group()
config.Data.outLFNDirBase = '/store/group/uhh/uhh2ntuples/RunII_106X_v1/%s/' % (year)

# If you want to run some private production and not put it in the group area, use this instead:
# replacing YOUR_CERN_USERNAME_HERE as appropriate
# config.Data.outLFNDirBase = '/store/user/YOUR_CERN_USERNAME_HERE/RunII_106X_v1/%s/' % (year)
if 'YOUR_CERN_USERNAME_HERE' in config.Data.outLFNDirBase:
    raise RuntimeError("You didn't insert your CERN username in config.Data.outLFNDirBase, please fix it")

config.Data.publication = False
config.JobType.sendExternalFolder = True
#config.Data.allowNonValidInputDataset = True

config.Site.storageSite = 'T2_DE_DESY'

if len(inputDatasets) > 0 and len(requestNames) > 0:
    config.General.requestName = requestNames[0]
    config.Data.inputDataset = inputDatasets[0]
