# This is a small example how the crab api can easily be used to create something like multi crab.
# It has some additional features like also creating the xml files for you.
# For it to work you need inputDatasets & requestNames apart from the classical part
#
# Make sure to have a unique directory where your joboutput is saved, otherwise the script gets confused and you too!!
#
# Usage ./CrabConfig ConfigFile [options]
#
# Take care here to make the request names *nice*
#
# autocomplete_Datasets(ListOfDatasets) works also for several entries with *


import re
from DasQuery import autocomplete_Datasets


def get_request_name(dataset_name):
    """Generate short string to use for request name from full dataset name

    Note that since this is used later on by e.g. multicrab status check,
    it should be invariant wrt time, commit hash, etc, otherwise it will not
    find the correct dir

    The short string generated by this function should however still contain as much information
    as needed so that people later on know which sample it is in CMS DAS (https://cmsweb.cern.ch/das)
    """

    is_real_data = dataset_name.split('/')[-1] == 'MINIAOD' # would be MINIAODSIM for MC
    # for real data, dataset names are usually not very long; thus, keep the full dataset name (except the obvious "MINIAOD" at the end) to prevent any confusion
    if is_real_data:
        modified_name = '_'.join(dataset_name.split('/')[1:-1])
        return modified_name

    # for the usually very long MC dataset names, do some more name modification ...

    # remove redundant substrings
    modified_name = dataset_name.split('/')[1].replace('Tune', '').replace('13TeV-', '').replace('13TeV_', '')

    campaign_etc = ''
    # Add MC campaign
    if "Summer16" in dataset_name:
        campaign_etc += "_Summer16"
    elif "Fall17" in dataset_name:
        campaign_etc += "_Fall17"
    elif "Autumn18" in dataset_name:
        campaign_etc += "_Autumn18"
    elif "Summer19UL17" in dataset_name:
        campaign_etc += "_Summer19UL17"
    elif "Summer19UL18" in dataset_name:
        campaign_etc += "_Summer19UL18"
    elif "Summer20UL16" in dataset_name:
        campaign_etc += "_Summer20UL16"
        if "APV" in dataset_name:
            campaign_etc += "APV"
    elif "Summer20UL17" in dataset_name:
        campaign_etc += "_Summer20UL17"
    elif "Summer20UL18" in dataset_name:
        campaign_etc += "_Summer20UL18"
    # Add 'ext1', 'ext2' etc.
    ext = re.search(r'ext[0-9]+', dataset_name.split('/')[-2])
    if ext:
        campaign_etc += '_'+ext.group(0)
    elif 'ext' in dataset_name:
        campaign_etc += '_ext'
    elif 'backup' in dataset_name:
        campaign_etc += '_backup'
    # Add sample version (last 'v*' before '/MINIAODSIM')
    version_number = dataset_name.split('/')[-2].split('-')[-1]
    if re.match(r'v[0-9]+', version_number):
        campaign_etc += '_'+version_number

    # request name can only be 100 characters maximum
    max_len = 100-len(campaign_etc)
    if len(modified_name) > max_len:
        modified_name = modified_name[:max_len]
    modified_name += campaign_etc

    return modified_name

def get_ntuplewriter(dataset, jetConstituents=False):
    ntuplewriter_name = 'ntuplewriter_'

    _,primary_ds_name,processed_ds_name,data_tier_name = tuple(dataset.split('/'))

    if(data_tier_name == 'MINIAODSIM'):
        ntuplewriter_name += 'mc_'
    elif(data_tier_name == 'MINIAOD'):
        ntuplewriter_name += 'data_'
    else:
        raise BaseException('Could not extract sample tier (MC;DATA) from DAS string:'%dataset)

    year = re.search(r'UL(?:20)?1[678]',processed_ds_name)
    if(year):
        ntuplewriter_name += year.group(0).replace('20','')
        if('16' in year.group(0)):
            if('APV' in processed_ds_name or 'HIPM' in processed_ds_name):
                ntuplewriter_name += 'preVFP'
            else:
                ntuplewriter_name += 'postVFP'
    else:
        raise BaseException('Could not extract year from DAS: %s'%dataset)
    
    if(jetConstituents):
        ntuplewriter_name += '_leadingjetConstits'

    ntuplewriter_name += '.py'
    return ntuplewriter_name


inputDatasets = ['/DYJetsToLL_M-50_HT-*to*_TuneCP5_PSweights_13TeV-madgraphMLM-pythia8/RunIISummer20UL16*APV*/MINIAODSIM']
inputDatasets = autocomplete_Datasets(inputDatasets)
requestNames = [get_request_name(x) for x in inputDatasets] # Here you can define custom request names if the get_request_names function doesn't return nice ones

if __name__=='__main__':
    print 'Request names (100 characters max.) will be:'
    for x in requestNames:
        print x.ljust(101), 'Length:', len(x)
    exit(0)

# ===============================================================================
# Classical part of crab, after resolving the * it uses in the example below just the first entry
#

from CRABClient.UserUtilities import config
from CRABClient.ClientExceptions import ProxyException
import os
import re


config = config()
config.General.workArea = 'crab_Test' # Don't forget to adjust this
config.General.transferOutputs = True
config.General.transferLogs = True

config.JobType.pluginName = 'Analysis'

# By default choose the ntuplewriter based on the DAS string
# set jetConstituents=True to take the version that stores the Constituents for the leading AK8 jets
# You can also choose the ntuplewriter yourself by replacing the function get_ntuplewriter with the name of the ntuplewriter_*_*.py
config.JobType.psetName = os.path.join(os.environ['CMSSW_BASE'], 'src/UHH2/core/python/', get_ntuplewriter(inputDatasets[0],jetConstituents=False))

config.JobType.outputFiles = ["Ntuple.root"]
config.JobType.maxMemoryMB = 2500

config.Data.inputDBS = 'global'
config.Data.splitting = 'EventAwareLumiBased' # (don't use the 'Automatic' splitting as recommended in the crab TWiki for real data; it sometimes bugs out and leads to duplicate data -- an absolute No-Go!)
config.Data.unitsPerJob = 24000 # targeted approximate number of events per job

# Add subdirectory using year from config filename
pset = os.path.basename(config.JobType.psetName)
result = re.search(r'(20|UL)1[\d](v\d)?', pset)
if not result:
    raise RuntimeError("Cannot extract year from psetName! Does your psetName have 201* in it (or UL1*)?")
year = result.group()
config.Data.outLFNDirBase = '/store/group/uhh/uhh2ntuples/RunII_106X_v1/%s/' % (year) # FIXME: change to RunII_106X_v2 before central production in autumn 2021

# If you want to run some private production and not put it in the group area, use this instead:
# replacing YOUR_CERN_USERNAME_HERE as appropriate
# config.Data.outLFNDirBase = '/store/user/YOUR_CERN_USERNAME_HERE/RunII_106X_v1/%s/' % (year) # FIXME: change to RunII_106X_v2 before central production in autumn 2021
if 'YOUR_CERN_USERNAME_HERE' in config.Data.outLFNDirBase:
    raise RuntimeError("You didn't insert your CERN username in config.Data.outLFNDirBase, please fix it")

config.Data.publication = False
config.JobType.sendExternalFolder = True
#config.Data.allowNonValidInputDataset = True

config.Site.storageSite = 'T2_DE_DESY'

if len(inputDatasets) > 0 and len(requestNames) > 0:
    config.General.requestName = requestNames[0]
    config.Data.inputDataset = inputDatasets[0]
